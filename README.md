# Multimodal-Sentiment-Analysis
This project involves researching ways to find a correlation between what people say and the facial expressions they make when saying it. The purpose of this research is to help boost the accuracy of sentiment prediction by also analyzing the facial features along with the person's speech. 

The files with 'translated' in the name require subscription to IBM Bluemix Language Translator API.

MOUD_text_production contains code for text analysis on the dataset and MOUD_text_Video_production contains analysis with the video features added.

Results and presentation is attached below.
![Alt text](/August%20update/August%20Update%20Multimodal%20sentiment%20analysis-1.jpg?raw=true "August Update Multimodal sentiment analysis-1")
![Alt text](/August%20update/August%20Update%20Multimodal%20sentiment%20analysis-2.jpg?raw=true "August Update Multimodal sentiment analysis-2")
![Alt text](/August%20update/August%20Update%20Multimodal%20sentiment%20analysis-3.jpg?raw=true "August Update Multimodal sentiment analysis-3")
![Alt text](/August%20update/August%20Update%20Multimodal%20sentiment%20analysis-4.jpg?raw=true "August Update Multimodal sentiment analysis-4")
![Alt text](/August%20update/August%20Update%20Multimodal%20sentiment%20analysis-5.jpg?raw=true "August Update Multimodal sentiment analysis-5")
